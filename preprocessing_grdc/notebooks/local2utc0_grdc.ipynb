{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 1. Read metadata and catchment area information\n",
    "# -----------------------------------------------------------\n",
    "# Read gauge coordinate metadata from all CSV files in GRDC_csv directory\n",
    "input_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv'\n",
    "output_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_utc0'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "gauge_coords = {}\n",
    "gauge_area = {}\n",
    "\n",
    "for file_path in glob.glob(os.path.join(input_dir, '*.csv')):\n",
    "    gauge_id = os.path.basename(file_path).split('.')[0]\n",
    "    df = pd.read_csv(file_path, nrows=3)  # Read only the first row for metadata\n",
    "    if df.empty or len(df) == 1:\n",
    "        print(f\"File {file_path} is empty or has only the header. Skipping.\")\n",
    "        continue\n",
    "    lat, lon, area = df['lat'].values[0], df['lon'].values[0], df['area'].values[0]\n",
    "    gauge_coords[gauge_id] = (lat, lon)\n",
    "    gauge_area[gauge_id] = area\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 2. Determine each gauge's local time zone offset (in hours)\n",
    "# -----------------------------------------------------------\n",
    "tf = TimezoneFinder()\n",
    "gauge_tz_offset = {}  # offset in hours\n",
    "rep_date = datetime.datetime(2020, 1, 1)  # representative date (adjust if needed)\n",
    "\n",
    "for gauge_id, (lat, lon) in gauge_coords.items():\n",
    "    tz_str = tf.timezone_at(lng=lon, lat=lat)\n",
    "    if tz_str is None:\n",
    "        print(f\"Timezone not found for {gauge_id} (lat={lat}, lon={lon}). Using UTC (offset=0).\")\n",
    "        gauge_tz_offset[gauge_id] = 0\n",
    "    else:\n",
    "        tz = pytz.timezone(tz_str)\n",
    "        localized = tz.localize(rep_date, is_dst=False)\n",
    "        offset_hours = localized.utcoffset().total_seconds() / 3600\n",
    "        gauge_tz_offset[gauge_id] = offset_hours\n",
    "        print(f\"Gauge {gauge_id}: Timezone {tz_str}, Offset {offset_hours} hours.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Step 3. Process each gauge's CSV file to shift daily streamflow to UTC \n",
    "#         and convert units from mm/day to m3/s.\n",
    "#\n",
    "#   (a) We load the gauge CSV (with dates and streamflow in mm/day) -- NOT NEED TO DO THIS FOR GRDC original dataset\n",
    "#   (b) We reindex the data to a complete daily time series.\n",
    "#   (c) We detect gaps. For gaps shorter than 7 days, we fill missing days\n",
    "#       via linear interpolation; for longer gaps, we treat the data as separate segments;\n",
    "#       ### if the total missing gap days exceed 1500, we skip the CSV file.\n",
    "#   (d) We remove catchment areas smaller than 500 km².\n",
    "#   (e) For each continuous segment, we apply the weighted average conversion:\n",
    "#\n",
    "#       For UTC+X (X>0):\n",
    "#         Q_UTC(D) = ((24-X) * Q_local(D) + X * Q_local(D+1)) / 24\n",
    "#\n",
    "#       For UTC-X (X>0):\n",
    "#         Q_UTC(D) = (X * Q_local(D-1) + (24-X) * Q_local(D)) / 24\n",
    "#\n",
    "#       (The first or last day of each segment is skipped accordingly.)\n",
    "#\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "input_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv'\n",
    "output_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_utc0_2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input directory.\n",
    "#all_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "#for file_path in all_files[:10]:\n",
    "\n",
    "for file_path in glob.glob(os.path.join(input_dir, '*.csv')):\n",
    "    gauge_id = os.path.basename(file_path).split('.')[0]\n",
    "    print(f\"\\nProcessing gauge {gauge_id} ...\")\n",
    "    \n",
    "    # Load the time series data with date parsing.\n",
    "    df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "\n",
    "    df.sort_values('date', inplace=True)\n",
    "    # Filter to only include data from January 1, 1979 onward.\n",
    "    df = df[df['date'] >= pd.Timestamp('1979-01-01')]\n",
    "\n",
    "    # Filter out rows with streamflow value equal to -999\n",
    "    df = df[df['streamflow'] != -999]\n",
    "\n",
    "    if df.empty or len(df) == 1:  # Skip empty files or files with only the header\n",
    "        print(f\"File {file_path} is empty or has only the header after 1979. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    #df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Ensure we have the 'streamflow' column.\n",
    "    if 'streamflow' not in df.columns:\n",
    "        print(f\"File {file_path} does not have a 'streamflow' column. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Set the date as index.\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Create a complete daily date range spanning the observed period.\n",
    "    full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df_full = df.reindex(full_index)\n",
    "    # Mark which dates are originally observed.\n",
    "    df_full['observed'] = ~df_full['streamflow'].isna()\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # Identify continuous segments.\n",
    "    # We want to group days where the gap between consecutive observed dates is less than 7 days.\n",
    "    # -------------------------------------------------------\n",
    "    observed_dates = df_full.index[df_full['observed']]\n",
    "    if len(observed_dates) == 0:\n",
    "        print(f\"No observed dates for {gauge_id}. Skipping file.\")\n",
    "        continue\n",
    "    total_gap_days = 0\n",
    "    gap_count = 0\n",
    "    segments = []\n",
    "    seg_start = observed_dates[0]\n",
    "    prev_date = observed_dates[0]\n",
    "    \n",
    "    for current_date in observed_dates[1:]:\n",
    "        gap = (current_date - prev_date).days\n",
    "        # Count missing days: if gap > 1 then missing days = gap - 1\n",
    "        if gap > 1:\n",
    "            total_gap_days += (gap - 1)\n",
    "        if gap < 8:  # if gap is less than 7 days, consider it continuous\n",
    "            prev_date = current_date\n",
    "        else:\n",
    "            print(f\"Gap detected: {gap} days between {prev_date.strftime('%Y-%m-%d')} and {current_date.strftime('%Y-%m-%d')}\")\n",
    "            gap_count += 1\n",
    "            segments.append((seg_start, prev_date))\n",
    "            seg_start = current_date\n",
    "            prev_date = current_date\n",
    "\n",
    "    # Append the last segment.\n",
    "    segments.append((seg_start, prev_date))\n",
    "    print(f\"Total gaps detected in {gauge_id}: {gap_count}\")\n",
    "    print(f\"Total missing gap days in {gauge_id}: {total_gap_days} days\")\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Skip file if catchment area is smaller than 500 km².\n",
    "    # make it comparable with our 5km resolution model output.\n",
    "    # -------------------------------------------------------\n",
    "    area = gauge_area.get(gauge_id)\n",
    "    if area is None:\n",
    "        print(f\"Catchment area not found for {gauge_id}. Skipping unit conversion.\")\n",
    "        continue\n",
    "    if area < 500:\n",
    "        print(f\"Skipping file {gauge_id} because catchment area ({area} km²) is smaller than 500 km².\")\n",
    "        continue\n",
    "    \n",
    "    # For each segment, reindex with daily frequency and fill gaps by linear interpolation.\n",
    "    converted_segments = []\n",
    "    for seg_start, seg_end in segments:\n",
    "        seg_index = pd.date_range(start=seg_start, end=seg_end, freq='D')\n",
    "        seg_df = df_full.loc[seg_start:seg_end].reindex(seg_index)\n",
    "        # Only fill gaps if they exist; if the gap is small (<7 days) this will interpolate.\n",
    "        seg_df['streamflow'] = seg_df['streamflow'].interpolate(method='linear')\n",
    "        seg_df = seg_df.copy()  # work on a copy\n",
    "        \n",
    "        # Apply the time conversion using the gauge's UTC offset.\n",
    "        offset = gauge_tz_offset.get(gauge_id, 0)\n",
    "        if offset > 0:\n",
    "            # For UTC+X, compute Q_UTC for day D using Q_local(D) and Q_local(D+1)\n",
    "            seg_df['streamflow_next'] = seg_df['streamflow'].shift(-1)\n",
    "            seg_df['Q_utc'] = ((24 - offset) * seg_df['streamflow'] + offset * seg_df['streamflow_next']) / 24.0\n",
    "            # Remove the last day of the segment (cannot combine with next day)\n",
    "            seg_df = seg_df.iloc[:-1]\n",
    "        elif offset < 0:\n",
    "            # For UTC-X, compute Q_UTC for day D using Q_local(D-1) and Q_local(D)\n",
    "            seg_df['streamflow_prev'] = seg_df['streamflow'].shift(1)\n",
    "            abs_offset = abs(offset)\n",
    "            seg_df['Q_utc'] = (abs_offset * seg_df['streamflow_prev'] + (24 - abs_offset) * seg_df['streamflow']) / 24.0\n",
    "            # Remove the first day of the segment\n",
    "            seg_df = seg_df.iloc[1:]\n",
    "        else:\n",
    "            seg_df['Q_utc'] = seg_df['streamflow']\n",
    "        \n",
    "        # Keep only the computed Q_utc and the date index.\n",
    "        converted_segments.append(seg_df[['Q_utc']])\n",
    "    \n",
    "    if not converted_segments:\n",
    "        print(f\"No valid segments found for {gauge_id}.\")\n",
    "        continue\n",
    "\n",
    "    # Combine all segments into one DataFrame (they remain separate in time).\n",
    "    df_conv = pd.concat(converted_segments).sort_index()\n",
    "\n",
    "    df_conv['streamflow'] = df_conv['Q_utc']\n",
    "\n",
    "    # Round the streamflow values to 3 decimal places\n",
    "    df_conv['streamflow'] = df_conv['streamflow'].round(3)\n",
    "    \n",
    "    # Prepare the output DataFrame (date and converted streamflow in m3/s).\n",
    "    df_out = df_conv[['streamflow']].reset_index().rename(columns={'index': 'date'})\n",
    "    # -------------------------------------------------------\n",
    "    # Append gauge metadata: latitude, longitude, and catchment area.\n",
    "    # -------------------------------------------------------\n",
    "    if gauge_id in gauge_coords:\n",
    "        lat, lon = gauge_coords[gauge_id]\n",
    "    else:\n",
    "        lat, lon = np.nan, np.nan\n",
    "    df_out['lat'] = lat\n",
    "    df_out['lon'] = lon\n",
    "    df_out['area'] = area\n",
    "    \n",
    "    # Save to a new CSV file (same gauge id as file name).\n",
    "    output_file = os.path.join(output_dir, f\"{gauge_id}.csv\")\n",
    "    df_out.to_csv(output_file, index=False)\n",
    "    print(f\"Processed {gauge_id}: converted data saved to {output_file}\")\n",
    "\n",
    "print(\"\\nAll files have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 866 files to /p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/converted_csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the directories\n",
    "dir1 = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_utc0'\n",
    "dir2 = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_utc0_2'\n",
    "output_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/converted_csv'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get the list of files in each directory\n",
    "files_in_dir1 = set(os.listdir(dir1))\n",
    "files_in_dir2 = set(os.listdir(dir2))\n",
    "\n",
    "# Find files that are in dir2 but not in dir1\n",
    "unique_files_in_dir2 = files_in_dir2 - files_in_dir1\n",
    "\n",
    "# Copy the unique files to the output directory\n",
    "for file in unique_files_in_dir2:\n",
    "    src_file = os.path.join(dir2, file)\n",
    "    dst_file = os.path.join(output_dir, file)\n",
    "    shutil.copy(src_file, dst_file)\n",
    "\n",
    "print(f\"Copied {len(unique_files_in_dir2)} files to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in /p/largedata2/detectdata/CentralDB/projects/d05/working_directory/NeuralFAS/GRDC_csv_utc0: 4658\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory\n",
    "converted_dir = '/p/largedata2/detectdata/CentralDB/projects/d05/working_directory/NeuralFAS/GRDC_csv_utc0'\n",
    "\n",
    "# Count the number of files in the directory\n",
    "num_files = len([name for name in os.listdir(converted_dir) if os.path.isfile(os.path.join(converted_dir, name))])\n",
    "\n",
    "print(f\"Number of files in {converted_dir}: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished converting GRDC_6836310 to UTC0. Saved to /p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_utc0/GRDC_6836310_new.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Input and output settings\n",
    "input_file = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv/GRDC_6836310.csv'\n",
    "converted_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_utc0'\n",
    "os.makedirs(converted_dir, exist_ok=True)\n",
    "\n",
    "# UTC offset and metadata (you must define these)\n",
    "gauge_id = 'GRDC_6836310'\n",
    "gauge_tz_offset = {'GRDC_6836310': 8}  # example offset: UTC+8\n",
    "gauge_coords = {'GRDC_6836310': (35.6, 103.8)}  # example lat, lon\n",
    "gauge_area = {'GRDC_6836310': 1234.5}  # example area in km²\n",
    "\n",
    "# Load and filter data\n",
    "df = pd.read_csv(input_file, parse_dates=['date'])\n",
    "df = df[df['date'] >= pd.Timestamp('1979-01-01')]\n",
    "df = df[df['streamflow'] != -999]\n",
    "if df.empty or len(df) == 1:\n",
    "    print(f\"Skipping {gauge_id}; no valid data.\")\n",
    "    exit()\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "df = df.sort_index()\n",
    "\n",
    "# Reindex to full daily range\n",
    "full_index = pd.date_range(df.index.min(), df.index.max(), freq='D')\n",
    "df_full = df.reindex(full_index)\n",
    "df_full['observed'] = ~df_full['streamflow'].isna()\n",
    "\n",
    "# Segment the time series\n",
    "observed_dates = df_full.index[df_full['observed']]\n",
    "if len(observed_dates) == 0:\n",
    "    print(f\"No observed data for {gauge_id}\")\n",
    "    exit()\n",
    "\n",
    "segments = []\n",
    "seg_start = prev_date = observed_dates[0]\n",
    "for current_date in observed_dates[1:]:\n",
    "    gap = (current_date - prev_date).days\n",
    "    if gap < 8:\n",
    "        prev_date = current_date\n",
    "    else:\n",
    "        segments.append((seg_start, prev_date))\n",
    "        seg_start = current_date\n",
    "        prev_date = current_date\n",
    "segments.append((seg_start, prev_date))\n",
    "\n",
    "# Check area threshold\n",
    "area = gauge_area[gauge_id]\n",
    "if area < 500:\n",
    "    print(f\"Skipping {gauge_id}; catchment area {area} km² < 500.\")\n",
    "    exit()\n",
    "\n",
    "# UTC conversion\n",
    "offset = gauge_tz_offset.get(gauge_id, 0)\n",
    "converted_segments = []\n",
    "\n",
    "for seg_start, seg_end in segments:\n",
    "    seg_index = pd.date_range(start=seg_start, end=seg_end, freq='D')\n",
    "    seg_df = df_full.loc[seg_start:seg_end].reindex(seg_index)\n",
    "    seg_df['streamflow'] = seg_df['streamflow'].interpolate(method='linear')\n",
    "    seg_df = seg_df.copy()\n",
    "\n",
    "    if offset > 0:\n",
    "        seg_df['streamflow_next'] = seg_df['streamflow'].shift(-1)\n",
    "        seg_df['Q_utc'] = ((24 - offset) * seg_df['streamflow'] + offset * seg_df['streamflow_next']) / 24.0\n",
    "        seg_df = seg_df.iloc[:-1]\n",
    "    elif offset < 0:\n",
    "        seg_df['streamflow_prev'] = seg_df['streamflow'].shift(1)\n",
    "        abs_offset = abs(offset)\n",
    "        seg_df['Q_utc'] = (abs_offset * seg_df['streamflow_prev'] + (24 - abs_offset) * seg_df['streamflow']) / 24.0\n",
    "        seg_df = seg_df.iloc[1:]\n",
    "    else:\n",
    "        seg_df['Q_utc'] = seg_df['streamflow']\n",
    "\n",
    "    converted_segments.append(seg_df[['Q_utc']])\n",
    "\n",
    "# Final output\n",
    "if not converted_segments:\n",
    "    print(f\"No valid segments for {gauge_id}\")\n",
    "    exit()\n",
    "\n",
    "df_conv = pd.concat(converted_segments).sort_index()\n",
    "df_conv['streamflow'] = df_conv['Q_utc'].round(3)\n",
    "\n",
    "df_out = df_conv[['streamflow']].reset_index().rename(columns={'index': 'date'})\n",
    "lat, lon = gauge_coords.get(gauge_id, (np.nan, np.nan))\n",
    "df_out['lat'] = lat\n",
    "df_out['lon'] = lon\n",
    "df_out['area'] = area\n",
    "\n",
    "output_path = os.path.join(converted_dir, f\"{gauge_id}_new.csv\")\n",
    "df_out.to_csv(output_path, index=False)\n",
    "print(f\"Finished converting {gauge_id} to UTC0. Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only filter the stations but without time conversion\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pytz\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Step 1. Read metadata and catchment area information\n",
    "# -----------------------------------------------------------\n",
    "# Read gauge coordinate metadata from all CSV files in GRDC_csv directory\n",
    "input_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv'\n",
    "output_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_filtered'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "gauge_coords = {}\n",
    "gauge_area = {}\n",
    "\n",
    "for file_path in glob.glob(os.path.join(input_dir, '*.csv')):\n",
    "    gauge_id = os.path.basename(file_path).split('.')[0]\n",
    "    df = pd.read_csv(file_path, nrows=3)  # Read only the first row for metadata\n",
    "    if df.empty or len(df) == 1:\n",
    "        print(f\"File {file_path} is empty or has only the header. Skipping.\")\n",
    "        continue\n",
    "    lat, lon, area = df['lat'].values[0], df['lon'].values[0], df['area'].values[0]\n",
    "    gauge_coords[gauge_id] = (lat, lon)\n",
    "    gauge_area[gauge_id] = area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Step 3. convert units from mm/day to m3/s.\n",
    "#\n",
    "#   (a) We load the gauge CSV (with dates and streamflow in mm/day) -- NOT NEED TO DO THIS FOR GRDC original dataset\n",
    "#   (b) We reindex the data to a complete daily time series.\n",
    "#   (c) We detect gaps. For gaps shorter than 7 days, we fill missing days\n",
    "#       via linear interpolation; for longer gaps, we treat the data as separate segments;\n",
    "#       ### if the total missing gap days exceed 1500, we skip the CSV file.\n",
    "#   (d) We remove catchment areas smaller than 500 km².\n",
    "#\n",
    "#       (The first or last day of each segment is skipped accordingly.)\n",
    "#\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "input_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv'\n",
    "output_dir = '/p/scratch/cesmtst/zhang36/NeuralFAS_dataset/GRDC_Caravan/GRDC_csv_filtered_2'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each CSV file in the input directory.\n",
    "#all_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
    "#for file_path in all_files[:10]:\n",
    "\n",
    "for file_path in glob.glob(os.path.join(input_dir, '*.csv')):\n",
    "    gauge_id = os.path.basename(file_path).split('.')[0]\n",
    "    print(f\"\\nProcessing gauge {gauge_id} ...\")\n",
    "    \n",
    "    # Load the time series data with date parsing.\n",
    "    df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "\n",
    "    df.sort_values('date', inplace=True)\n",
    "    # Filter to only include data from January 1, 1979 onward.\n",
    "    df = df[df['date'] >= pd.Timestamp('1979-01-01')]\n",
    "\n",
    "    # Filter out rows with streamflow value equal to -999\n",
    "    df = df[df['streamflow'] != -999]\n",
    "\n",
    "    if df.empty or len(df) == 1:  # Skip empty files or files with only the header\n",
    "        print(f\"File {file_path} is empty or has only the header after 1979. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    #df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Ensure we have the 'streamflow' column.\n",
    "    if 'streamflow' not in df.columns:\n",
    "        print(f\"File {file_path} does not have a 'streamflow' column. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Set the date as index.\n",
    "    df.set_index('date', inplace=True)\n",
    "    \n",
    "    # Create a complete daily date range spanning the observed period.\n",
    "    full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='D')\n",
    "    df_full = df.reindex(full_index)\n",
    "    # Mark which dates are originally observed.\n",
    "    df_full['observed'] = ~df_full['streamflow'].isna()\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # Identify continuous segments.\n",
    "    # We want to group days where the gap between consecutive observed dates is less than 7 days.\n",
    "    # -------------------------------------------------------\n",
    "    observed_dates = df_full.index[df_full['observed']]\n",
    "    if len(observed_dates) == 0:\n",
    "        print(f\"No observed dates for {gauge_id}. Skipping file.\")\n",
    "        continue\n",
    "    total_gap_days = 0\n",
    "    gap_count = 0\n",
    "    segments = []\n",
    "    seg_start = observed_dates[0]\n",
    "    prev_date = observed_dates[0]\n",
    "    \n",
    "    for current_date in observed_dates[1:]:\n",
    "        gap = (current_date - prev_date).days\n",
    "        # Count missing days: if gap > 1 then missing days = gap - 1\n",
    "        if gap > 1:\n",
    "            total_gap_days += (gap - 1)\n",
    "        if gap < 8:  # if gap is less than 7 days, consider it continuous\n",
    "            prev_date = current_date\n",
    "        else:\n",
    "            print(f\"Gap detected: {gap} days between {prev_date.strftime('%Y-%m-%d')} and {current_date.strftime('%Y-%m-%d')}\")\n",
    "            gap_count += 1\n",
    "            segments.append((seg_start, prev_date))\n",
    "            seg_start = current_date\n",
    "            prev_date = current_date\n",
    "\n",
    "    # Append the last segment.\n",
    "    segments.append((seg_start, prev_date))\n",
    "    print(f\"Total gaps detected in {gauge_id}: {gap_count}\")\n",
    "    print(f\"Total missing gap days in {gauge_id}: {total_gap_days} days\")\n",
    "    \n",
    "    # If total missing gap days exceed 1500, skip this CSV file.\n",
    "    if total_gap_days > 1500:\n",
    "        print(f\"Skipping file {gauge_id} because total missing gap days ({total_gap_days}) exceed 10% days between 1979 to 2022.\")\n",
    "        continue\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # Skip file if catchment area is smaller than 500 km².\n",
    "    # make it comparable with our 5km resolution model output.\n",
    "    # -------------------------------------------------------\n",
    "    area = gauge_area.get(gauge_id)\n",
    "    if area is None:\n",
    "        print(f\"Catchment area not found for {gauge_id}. Skipping unit conversion.\")\n",
    "        continue\n",
    "    if area < 500:\n",
    "        print(f\"Skipping file {gauge_id} because catchment area ({area} km²) is smaller than 500 km².\")\n",
    "        continue\n",
    "    \n",
    "    # For each segment, reindex with daily frequency and fill gaps by linear interpolation.\n",
    "    converted_segments = []\n",
    "    for seg_start, seg_end in segments:\n",
    "        seg_index = pd.date_range(start=seg_start, end=seg_end, freq='D')\n",
    "        seg_df = df_full.loc[seg_start:seg_end].reindex(seg_index)\n",
    "        # Only fill gaps if they exist; if the gap is small (<7 days) this will interpolate.\n",
    "        seg_df['streamflow'] = seg_df['streamflow'].interpolate(method='linear')\n",
    "        seg_df = seg_df.copy()  # work on a copy\n",
    "        \n",
    "        # Keep only the computed Q_utc and the date index.\n",
    "        converted_segments.append(seg_df[['streamflow']])\n",
    "    \n",
    "    if not converted_segments:\n",
    "        print(f\"No valid segments found for {gauge_id}.\")\n",
    "        continue\n",
    "\n",
    "    # Combine all segments into one DataFrame (they remain separate in time).\n",
    "    df_conv = pd.concat(converted_segments).sort_index()\n",
    "\n",
    "    df_conv['streamflow'] = df_conv['streamflow']\n",
    "\n",
    "    # Round the streamflow values to 3 decimal places\n",
    "    df_conv['streamflow'] = df_conv['streamflow'].round(3)\n",
    "    \n",
    "    # Prepare the output DataFrame (date and converted streamflow in m3/s).\n",
    "    df_out = df_conv[['streamflow']].reset_index().rename(columns={'index': 'date'})\n",
    "    # -------------------------------------------------------\n",
    "    # Append gauge metadata: latitude, longitude, and catchment area.\n",
    "    # -------------------------------------------------------\n",
    "    if gauge_id in gauge_coords:\n",
    "        lat, lon = gauge_coords[gauge_id]\n",
    "    else:\n",
    "        lat, lon = np.nan, np.nan\n",
    "    df_out['lat'] = lat\n",
    "    df_out['lon'] = lon\n",
    "    df_out['area'] = area\n",
    "    \n",
    "    # Save to a new CSV file (same gauge id as file name).\n",
    "    output_file = os.path.join(output_dir, f\"{gauge_id}.csv\")\n",
    "    df_out.to_csv(output_file, index=False)\n",
    "    print(f\"Processed {gauge_id}: converted data saved to {output_file}\")\n",
    "\n",
    "print(\"\\nAll files have been processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralfas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
